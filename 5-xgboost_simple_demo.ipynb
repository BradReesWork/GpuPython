{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost - Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost is a supervised learning algorithm that implements a process called boosting to yield accurate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This automatically time every cell's execution\n",
    "!pip install ipython-autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dataset using sklearn\n",
    "cov = fetch_covtype()\n",
    "X = cov.data\n",
    "y = cov.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 0.75/0.25 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, train_size=0.75,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input data from numpy to XGBoost format\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "num_round = 10\n",
    "maxdepth = 6\n",
    "# base parameters\n",
    "param = {'tree_method': 'gpu_hist',\n",
    "         'grow_policy': 'depthwise',\n",
    "         'max_depth': maxdepth,\n",
    "         'random_state': 1234,\n",
    "         'objective': 'multi:softmax', # Specify multiclass classification\n",
    "         'num_class': 8, # Number of possible output classes\n",
    "         'base_score': 0.5,\n",
    "         'booster': 'gbtree',\n",
    "         'colsample_bylevel': 1,\n",
    "         'colsample_bytree': 1,\n",
    "         'gamma': 0,\n",
    "         'learning_rate': 0.1, \n",
    "         'max_delta_step': 0,\n",
    "         'min_child_weight': 1,\n",
    "         'missing': None,\n",
    "         'n_estimators': 3,\n",
    "         'scale_pos_weight': 1,\n",
    "         'silent': True,\n",
    "         'subsample': 1,\n",
    "         'verbose': True,\n",
    "         'n_jobs': -1\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU HIST DEPTHWISE\n",
    "param['tree_method'] = 'gpu_hist'\n",
    "param['grow_policy'] = 'depthwise'\n",
    "param['max_depth'] = maxdepth\n",
    "param['max_leaves'] = 0\n",
    "gpu_res = {} # Store accuracy result\n",
    "tmp = time.time()\n",
    "# Train model\n",
    "xgb.train(param, dtrain, num_round, evals=[(dtest, 'test')], evals_result=gpu_res)\n",
    "print(\"GPU Training Time: %s seconds\" % (str(time.time() - tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU HIST LOSSGUIDE\n",
    "param['tree_method'] = 'gpu_hist'\n",
    "param['grow_policy'] = 'lossguide'\n",
    "param['max_depth'] = 0\n",
    "param['max_leaves'] = np.power(2,maxdepth)\n",
    "gpu_res = {} # Store accuracy result\n",
    "tmp = time.time()\n",
    "# Train model\n",
    "xgb.train(param, dtrain, num_round, evals=[(dtest, 'test')], evals_result=gpu_res)\n",
    "print(\"GPU Training Time: %s seconds\" % (str(time.time() - tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU HIST DEPTHWISE\n",
    "param['tree_method'] = 'hist'\n",
    "param['grow_policy'] = 'depthwise'\n",
    "param['max_depth'] = maxdepth\n",
    "param['max_leaves'] = 0\n",
    "cpu_res = {} # Store accuracy result\n",
    "tmp = time.time()\n",
    "# Train model\n",
    "xgb.train(param, dtrain, num_round, evals=[(dtest, 'test')], evals_result=cpu_res)\n",
    "print(\"CPU Training Time: %s seconds\" % (str(time.time() - tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU HIST LOSSGUIDE\n",
    "param['tree_method'] = 'hist'\n",
    "param['grow_policy'] = 'lossguide'\n",
    "param['max_depth'] = 0\n",
    "param['max_leaves'] = np.power(2,maxdepth)\n",
    "cpu_res = {} # Store accuracy result\n",
    "tmp = time.time()\n",
    "# Train model\n",
    "xgb.train(param, dtrain, num_round, evals=[(dtest, 'test')], evals_result=cpu_res)\n",
    "print(\"CPU Training Time: %s seconds\" % (str(time.time() - tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLEARN API XGBOOST PARAMETERS\n",
    "kwargs = {'grow_policy': 'depthwise',\n",
    "          'eval_metric': 'mlogloss',\n",
    "          'num_class': 8,\n",
    "          'objective': 'multi:softmax',\n",
    "          'n_estimators': num_round,\n",
    "          'tree_method': \"gpu_hist\",\n",
    "          'grow_policy': 'depthwise',\n",
    "          'max_depth': maxdepth,\n",
    "          'random_state': 1234,\n",
    "          'n_jobs': -1,\n",
    "          'silent': True,\n",
    "          'debug_verbose': 2,\n",
    "          'verbose': True}\n",
    "eval_metric = 'mlogloss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLEARN GPU HIST DEPTHWISE\n",
    "kwargs['tree_method'] = 'gpu_hist'\n",
    "kwargs['grow_policy'] = \"depthwise\"\n",
    "kwargs['max_depth'] = maxdepth\n",
    "kwargs['max_leaves'] = 0\n",
    "model = xgb.XGBClassifier(**kwargs)\n",
    "model.fit(X=X_train, y=y_train, verbose=True, eval_set=[(X_test, y_test)], eval_metric=eval_metric)\n",
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLEARN GPU HIST LOSSGUIDE\n",
    "kwargs['tree_method'] = 'gpu_hist'\n",
    "kwargs['grow_policy'] = \"lossguide\"\n",
    "kwargs['max_depth'] = 0\n",
    "kwargs['max_leaves'] = np.power(2,maxdepth)\n",
    "model = xgb.XGBClassifier(**kwargs)\n",
    "model.fit(X=X_train, y=y_train, verbose=True, eval_set=[(X_test, y_test)], eval_metric=eval_metric)\n",
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLEARN CPU HIST DEPTHWISE\n",
    "kwargs['tree_method'] = 'hist'\n",
    "kwargs['grow_policy'] = \"depthwise\"\n",
    "kwargs['max_depth'] = maxdepth\n",
    "kwargs['max_leaves'] = 0\n",
    "model = xgb.XGBClassifier(**kwargs)\n",
    "model.fit(X=X_train, y=y_train, verbose=True, eval_set=[(X_test, y_test)], eval_metric=eval_metric)\n",
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKLEARN CPU HIST LOSSGUIDE\n",
    "kwargs['tree_method'] = 'hist'\n",
    "kwargs['grow_policy'] = \"lossguide\"\n",
    "kwargs['max_depth'] = 0\n",
    "kwargs['max_leaves'] = np.power(2,maxdepth)\n",
    "model = xgb.XGBClassifier(**kwargs)\n",
    "model.fit(X=X_train, y=y_train, verbose=True, eval_set=[(X_test, y_test)], eval_metric=eval_metric)\n",
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#\n",
    "# GPU DEMO of feature importance\n",
    "#\n",
    "###############################################\n",
    "# Prediction\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Test Set Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "# Retrieve performance metrics\n",
    "import matplotlib.pyplot as plt\n",
    "results = model.evals_result()\n",
    "epochs = len(results['validation_0']['mlogloss'])\n",
    "x_axis = range(0, epochs)\n",
    "# plot log loss\n",
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "ax.plot(x_axis, results['validation_0']['mlogloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['mlogloss'], label='Test')\n",
    "ax.legend()\n",
    "plt.ylabel('Multi-Class LogLoss')\n",
    "plt.title('XGBoost Multi-Class LogLoss')\n",
    "plt.show()\n",
    "\n",
    "# plot feature importance using built-in function\n",
    "from xgboost import plot_importance\n",
    "# plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "plot_importance(model, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new version using xgbfi-C++\n",
    "def calc_varimp(context, model, columns, how_many, n_trees, from_depth, to_depth, **kwargs):\n",
    "    t0 = time.time()\n",
    "    booster = model._Booster\n",
    "    booster.feature_names = columns\n",
    "\n",
    "    max_xgbfi_tree_depth = 7\n",
    "    max_xgbfi_tree_deepening = -1\n",
    "\n",
    "    params = model.get_params()\n",
    "    n_jobs = params['n_jobs']\n",
    "    print(\"n_jobs=%d\" % n_jobs)\n",
    "    df_imp = booster.get_feature_interactions(to_depth,\n",
    "                                              max_xgbfi_tree_depth,\n",
    "                                              max_xgbfi_tree_deepening,\n",
    "                                              n_trees,\n",
    "                                              nthread=n_jobs)\n",
    "    if df_imp.shape[0] == 0:\n",
    "        df_imp = try_calc_varimp_gblinear(booster=booster, columns=columns, **kwargs)\n",
    "\n",
    "    if df_imp.shape[0] == 0:\n",
    "        df_imp['fi'] = columns\n",
    "        df_imp['fi_depth'] = 0\n",
    "        df_imp['gain'] = 1.0\n",
    "\n",
    "    depth_from_to = (from_depth <= df_imp.fi_depth) & (df_imp.fi_depth <= to_depth)\n",
    "    df_imp = df_imp[['fi', 'fi_depth', 'gain']].loc[depth_from_to].reset_index(drop=True)\n",
    "    df_imp['fi_depth'] = df_imp['fi_depth'].astype(int)\n",
    "\n",
    "    df_imp_feats = df_imp.loc[df_imp.fi_depth == 0, 'fi'].values\n",
    "    missing_feats = np.setdiff1d(columns, df_imp_feats)\n",
    "    if len(missing_feats) > 0 and from_depth == 0:\n",
    "        df_missing_feats = pd.DataFrame()\n",
    "        df_missing_feats['fi'] = missing_feats\n",
    "        df_missing_feats['fi_depth'] = 0\n",
    "        df_missing_feats['gain'] = 0.0\n",
    "        df_imp = pd.concat((df_imp, df_missing_feats)) \\\n",
    "            .sort_values(by=['fi_depth', 'gain'], ascending=[True, False]) \\\n",
    "            .reset_index(drop=True)\n",
    "\n",
    "    df_grp = df_imp.groupby('fi_depth')\n",
    "    df_imp['gain'] = df_grp['gain'].transform(lambda x: (x / x.max()))\n",
    "    df_imp = df_grp.head(how_many).reset_index(drop=True)\n",
    "    df_imp.columns = ['Interaction', 'Depth', 'Gain']\n",
    "\n",
    "    t1 = time.time()\n",
    "    return df_imp\n",
    "\n",
    "\n",
    "def try_calc_varimp_gblinear(booster, columns, **kwargs):\n",
    "    df_imp = pd.DataFrame()\n",
    "    dump = booster.get_dump()[0]\n",
    "    if 'weight:' not in dump:\n",
    "        return df_imp\n",
    "    n_models = len(dump.split('\\nweight:')[0].split('\\n')[1:])\n",
    "    imp = []\n",
    "    for w in dump.split('\\n')[2 + n_models:-1]:\n",
    "        imp.append(abs(np.float(\"{:.6f}\".format(float(w)))))\n",
    "    imp = np.array(imp).reshape((-1, n_models)).mean(axis=1)\n",
    "    df_imp['fi'] = columns\n",
    "    df_imp['fi_depth'] = 0\n",
    "    df_imp['gain'] = imp\n",
    "    df_imp = df_imp.sort_values(by='gain', ascending=False).reset_index(drop=True)\n",
    "    return df_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context=None\n",
    "import pandas as pd\n",
    "pd_X_train = pd.DataFrame(X_train)\n",
    "booster = model._Booster\n",
    "#booster.feature_names = [x for x in str(pd_X_train.columns.values)]\n",
    "booster.feature_names = [\"c\"+str(x) for x in pd_X_train.columns.values]\n",
    "kwargs = {}\n",
    "# params: context, model, columns, N, n_trees, from_depth, to_depth, **kwargs)\n",
    "Nimportance = 450\n",
    "Ntrees = model.best_ntree_limit\n",
    "from_depth = 0\n",
    "to_depth = maxdepth\n",
    "imp_features = calc_varimp(context, model, booster.feature_names, Nimportance, Ntrees, from_depth, to_depth, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_features[imp_features.Depth==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_features[imp_features.Depth==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###############################################\n",
    "#\n",
    "# GPU DEMO of feature importance\n",
    "#\n",
    "###############################################\n",
    "num_round_more = 200\n",
    "maxdepth_more = 6\n",
    "# SKLEARN GPU HIST LOSSGUIDE\n",
    "kwargs['tree_method'] = 'gpu_hist'\n",
    "kwargs['grow_policy'] = \"lossguide\"\n",
    "kwargs['max_depth'] = 0\n",
    "kwargs['max_leaves'] = np.power(2,maxdepth_more)\n",
    "kwargs['n_estimators'] = num_round_more\n",
    "kwargs['n_jobs'] = 1\n",
    "model = xgb.XGBClassifier(**kwargs)\n",
    "eval_set = [(X_train, y_train),(X_test, y_test)]\n",
    "model.fit(X=X_train, y=y_train, verbose=True, eval_set=eval_set, eval_metric=eval_metric, early_stopping_rounds=20)\n",
    "print(\"Number of trees for best model: %d\" % model.best_ntree_limit)\n",
    "print(\"Model parameters: %s\" % str(model.get_params()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dofinalcputest=1\n",
    "if dofinalcputest==1:\n",
    "    ###############################################\n",
    "    #\n",
    "    # CPU Model (compare time to GPU DEMO of feature importance) .\n",
    "    #\n",
    "    ###############################################\n",
    "    # SKLEARN CPU HIST LOSSGUIDE\n",
    "    kwargs['tree_method'] = 'hist'\n",
    "    kwargs['grow_policy'] = \"lossguide\"\n",
    "    kwargs['max_depth'] = 0\n",
    "    kwargs['max_leaves'] = np.power(2,maxdepth_more)\n",
    "    kwargs['n_estimators'] = num_round_more\n",
    "    model = xgb.XGBClassifier(**kwargs)\n",
    "    eval_set = [(X_train, y_train),(X_test, y_test)]\n",
    "    model.fit(X=X_train, y=y_train, verbose=True, eval_set=eval_set, eval_metric=eval_metric, early_stopping_rounds=20)\n",
    "    print(\"Number of trees for best model: %d\" % model.best_ntree_limit)\n",
    "    print(\"Model parameters: %s\" % str(model.get_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
